{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71953d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparation code in Google Colab\n",
    "\n",
    "# Install required packages for the GAN training code\n",
    "!pip install --quiet torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 \\\n",
    "    --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "!pip install --quiet torchsummary==1.5.1\n",
    "\n",
    "# Optional but useful for progress bars (if you add tqdm later)\n",
    "!pip install --quiet tqdm\n",
    "\n",
    "!pip install --quiet torcheval\n",
    "\n",
    "print(\"Installation finished.\")\n",
    "import torch\n",
    "import torchvision\n",
    "print(\"torch:     \", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "print(\"torchsummary:\", \"1.5.1 (installed)\")\n",
    "print(\"torcheval:\", \"ready\")\n",
    "\n",
    "# link to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Unzip Dataset\n",
    "!unzip -q /content/drive/MyDrive/LEGObrick.zip -d /content/lego_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as Transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n",
    "from torcheval import metrics as Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/content/lego_data'\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 1 # Grayscale images\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 100\n",
    "NOISE_PARAM = 0.1\n",
    "EPOCHS = 200\n",
    "CRITIC_STEPS = 5\n",
    "LAMBDA = 10\n",
    "ADAM_BETA_1_C = 0.0\n",
    "ADAM_BETA_1_G = 0.5\n",
    "ADAM_BETA_2 = 0.9\n",
    "LR = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ee3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, generator, discriminator, g_optim, d_optim, history, path=\"/content/drive/MyDrive/dcgan_checkpoint.pth\"):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'g_optimizer_state_dict': g_optim.state_dict(),\n",
    "        'd_optimizer_state_dict': d_optim.state_dict(),\n",
    "        'g_scheduler_state_dict': g_scheduler.state_dict(),\n",
    "        'd_scheduler_state_dict': d_scheduler.state_dict(),\n",
    "        'history': history,\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"--- Checkpoint saved at epoch {epoch} to {path} ---\")\n",
    "\n",
    "def load_checkpoint(path, generator, discriminator, g_optim, d_optim):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Loading checkpoint from {path}...\")\n",
    "        checkpoint = torch.load(path)\n",
    "\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        g_optim.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "        d_optim.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "        g_scheduler.load_state_dict(checkpoint['g_scheduler_state_dict'])\n",
    "        d_scheduler.load_state_dict(checkpoint['d_scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        history = checkpoint['history']\n",
    "\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "        return start_epoch, history\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return 0, defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a994241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegoData(Dataset): # from torch.utils.data\n",
    "    def __init__(self, img_dir):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        # os.listdir is not for subfolders\n",
    "        #self.imgs = os.listdir(img_dir) # list of image file names\n",
    "        self.imgs = []\n",
    "        for root, dirs, files in os.walk(img_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    self.imgs.append(os.path.join(root, file))\n",
    "        self.length = len(self.imgs)\n",
    "\n",
    "        # Define Transformer\n",
    "        self.transform = Transforms.Compose([\n",
    "            Transforms.Grayscale(num_output_channels=CHANNELS),\n",
    "            Transforms.ToTensor(),\n",
    "            Transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            Transforms.Normalize((0.5,), (0.5,))]) # Normalize to [-1, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx): # enable indexing\n",
    "        img_file = self.imgs[idx]\n",
    "        image = Image.open(img_file)\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def get_dataloader():\n",
    "    lego_dataset = LegoData(DATA_DIR)\n",
    "    train_loader = DataLoader(lego_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = 0, pin_memory = True)\n",
    "        # pin_memory when we read data to CPU, it will be faster to transfer to GPU\n",
    "        # num_workers defines the number of parallel subprocesses used to load data (optimal: 2 * number of GPU cores, but not recommedned for Windows)\n",
    "    print('Train data size: ', len(lego_dataset))\n",
    "    print('Num. train batchs: ', len(train_loader))\n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3154d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_imgs(imgs):\n",
    "    plt.figure(figsize = (16, 3))\n",
    "    for i in range(8):\n",
    "        ax = plt.subplot(1, 8, i+1)\n",
    "        ax.imshow(imgs[i][0], cmap = 'gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_records(history):\n",
    "    plt.figure(figsize = (16, 3))\n",
    "    for i, key in enumerate(history):\n",
    "        ax = plt.subplot(1, len(history), i+1)\n",
    "        ax.plot(history[key])\n",
    "        ax.set_title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29061c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader()\n",
    "real_imgs = next(iter(train_loader))\n",
    "display_imgs(real_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a49782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channel_list = [CHANNELS, 64, 128, 256, 512]\n",
    "\n",
    "        self.conv_layers = nn.Sequential()\n",
    "        for i in range(len(channel_list) - 1):\n",
    "            conv_block = self.get_conv_block(channel_list[i], channel_list[i+1], bool(i))\n",
    "            self.conv_layers.add_module(name = f'conv_block_{i+1}', module = conv_block)\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = channel_list[-1], out_channels = 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    def get_conv_block(self, in_channels, out_channels, use_bn = True):\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace = False),\n",
    "            nn.Dropout2d(p = 0.3)\n",
    "        ]\n",
    "        if use_bn:\n",
    "            layers.insert(1, nn.BatchNorm2d(out_channels))\n",
    "        return nn.Sequential(*layers) # the * to unpack the list argument\n",
    "\n",
    "test = Critic().to(DEVICE)\n",
    "summary(test, (1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6343f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        channel_list = [z_dim, 512, 256, 128, 64]\n",
    "\n",
    "        self.conv_trans_layers = nn.Sequential()\n",
    "        for i in range(len(channel_list) - 1):\n",
    "            stride = 2 if i else 1\n",
    "            padding = 1 if i else 0\n",
    "            trans_conv_block = self.get_conv_trans_block(channel_list[i], channel_list[i+1], stride, padding)\n",
    "            self.conv_trans_layers.add_module(name = f'conv_trans_block_{i+1}',\n",
    "                                              module = trans_conv_block)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = channel_list[-1], out_channels = CHANNELS, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.reshape(x.shape[0], x.shape[-1], 1, 1)\n",
    "        x = self.conv_trans_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_conv_trans_block(self, in_channels, out_channels, stride = 2, padding = 1):\n",
    "        Layers = [\n",
    "            nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size=4, stride=stride, padding=padding, bias=False),\n",
    "            nn.InstanceNorm2d(num_features = out_channels, affine = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.2, inplace=False)\n",
    "        ]\n",
    "        return nn.Sequential(*Layers)\n",
    "\n",
    "generator = Generator(Z_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43145c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN_GP(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.generator = Generator(z_dim)\n",
    "        self.critic = Critic(real_imgs)\n",
    "        self.critic_step = CRITIC_STEPS\n",
    "\n",
    "\n",
    "    def generate(self, real_imgs):\n",
    "        latents = torch.randn(size = (len(real_imgs), self.z_dim, 1, 1)).to(DEVICE)\n",
    "        generated_imgs = self.generator(latents)\n",
    "        return generated_imgs\n",
    "    \n",
    "    def gradient_penalty(self, real_imgs, fake_imgs):\n",
    "        batch_size = real_imgs.size(0)\n",
    "        alpha = torch.rand(batch_size, 1, 1, 1).to(DEVICE) # random weight for interpolation\n",
    "        interpolated = alpha * real_imgs + (1 - alpha) * fake_imgs\n",
    "        interpolated.requires_grad_(True)\n",
    "\n",
    "        critic_interpolated = self.critic(interpolated)\n",
    "\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=critic_interpolated,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(critic_interpolated).to(DEVICE),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradient_norm = gradients.norm(2, dim=1)\n",
    "        penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "        return penalty\n",
    "\n",
    "wgan_gp = WGAN_GP(Z_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cdc9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan_gp = WGAN_GP(Z_DIM).to(DEVICE)\n",
    "\n",
    "if torch.__version__.split('.')[0] == '2':\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    # It is important to use eager backend here to avoid\n",
    "    # distribution mismatch in training and predicting\n",
    "    dcgan = torch.compile(dcgan, backend='eager')\n",
    "    print('model compiled')\n",
    "\n",
    "# loss function calculated manually in training loop\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "c_optim = torch.optim.Adam(params=wgan_gp.critic.parameters(), lr=LR, betas=(ADAM_BETA_1_C, ADAM_BETA_2))\n",
    "g_optim = torch.optim.Adam(params=wgan_gp.generator.parameters() ,lr=LR, betas=(ADAM_BETA_1_G, ADAM_BETA_2))\n",
    "\n",
    "\n",
    "train_metrics = {\n",
    "    'c_wass_loss': Metrics.Mean(),\n",
    "    'c_gp': Metrics.Mean(),\n",
    "    'c_loss': Metrics.Mean(),\n",
    "    'g_loss': Metrics.Mean()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa15582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, c_optim, g_optim, train_metrics):\n",
    "    model.train()\n",
    "\n",
    "    for metrics in train_metrics.values():\n",
    "        metrics.reset()\n",
    "\n",
    "    for i, train_imgs in enumerate(dataloader):\n",
    "        \n",
    "        #1 Update Discriminator\n",
    "        for j in range(model.critic_step):\n",
    "            model.critic.zero_grad()\n",
    "\n",
    "            # the real images and fake images\n",
    "            train_imgs = train_imgs.to(DEVICE)\n",
    "            generated_imgs = model.generate(train_imgs).detach() # detach to avoid backprop to generator\n",
    "\n",
    "            # calculate EM distance and gradient penalty\n",
    "            W_dist = model.critic(train_imgs).mean() - model.critic(generated_imgs).mean()\n",
    "            gp = model.gradient_penalty(train_imgs, generated_imgs)\n",
    "            critic_loss = -W_dist + LAMBDA * gp # lambda for gradient penalty is 10\n",
    "\n",
    "            critic_loss.backward()\n",
    "            c_optim.step()\n",
    "            \n",
    "\n",
    "        # 2 Update Generator\n",
    "        model.generator.zero_grad()\n",
    "\n",
    "        # generate new batch of fake images from latents\n",
    "        generated_imgs_fresh = model.generate(train_imgs)\n",
    "\n",
    "        # calculate generator loss\n",
    "        g_loss = -model.critic(generated_imgs_fresh).mean()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "\n",
    "        # 3 Update Metrics\n",
    "        train_metrics['c_wass_loss'].update(-W_dist.detach().cpu())\n",
    "        train_metrics['c_gp'].update(gp.detach().cpu())\n",
    "        train_metrics['g_loss'].update(g_loss.detach().cpu())\n",
    "        train_metrics['c_loss'].update(critic_loss.detach().cpu())\n",
    "\n",
    "        # del d_loss, g_loss, real_preds, fake_preds, real_labels, fake_labels, real_noisy_labels, fake_noisy_labels\n",
    "    return generated_imgs_fresh.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecdda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lego_dataloader = get_dataloader()\n",
    "history = defaultdict(list)\n",
    "\n",
    "# for model checkpoint\n",
    "CHECKPOINT_PATH = '/content/drive/MyDrive/lego_wgan_gp_ckpt.pth'\n",
    "start_epoch, history = load_checkpoint(CHECKPOINT_PATH, wgan_gp.generator, wgan_gp.critic, g_optim, c_optim)\n",
    "\n",
    "# for logger\n",
    "writer = SummaryWriter('/content/drive/MyDrive/runs/lego_experiment_2')\n",
    "\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    prev_time = time.time()\n",
    "    generated_imgs = train(wgan_gp, lego_dataloader, c_optim, g_optim, train_metrics)\n",
    "    curr_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    for key, value in train_metrics.items():\n",
    "        history[key].append(value.compute().item())\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    print('Epoch: {}\\tepoch time {:.2f} min'.format(i+1, (curr_time - prev_time) / 60))\n",
    "    metrics = [f'{key}: {value.compute().item():.4f} | ' for key, value in train_metrics.items()]\n",
    "    print('\\t', ''.join(metrics))\n",
    "\n",
    "    generated_imgs = generated_imgs.detach().cpu()\n",
    "    display_imgs(generated_imgs)  # denormalize to [0, 1] for display\n",
    "    show_records(history)\n",
    "    writer.add_scalar('Wasserstein Loss/Critic', train_metrics['wass_loss'].compute(), i)\n",
    "    writer.add_scalar('Gradient Penalty/Critic', train_metrics['c_gp'].compute(), i)\n",
    "    writer.add_scalar('Loss/Generator', train_metrics['g_loss'].compute(), i)\n",
    "    writer.add_scalar('Loss/Critic', train_metrics['c_loss'].compute(), i)\n",
    "\n",
    "    if (i + 1) % 5 == 0:\n",
    "        save_checkpoint(i, wgan_gp.generator, wgan_gp.critic, g_optim, c_optim, history, CHECKPOINT_PATH)\n",
    "        # Use make_grid to see a batch of LEGOs at once\n",
    "        grid = torchvision.utils.make_grid(generated_imgs[:16], normalize=True)\n",
    "        writer.add_image('Generated_Bricks', grid, i)\n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample lantent vector from normal distribution\n",
    "grid_width, grid_height = (10, 3)\n",
    "z_samples = torch.randn(size=(grid_width * grid_height, Z_DIM), device=DEVICE)\n",
    "generated_imgs = dcgan.generate(z_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafada4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a plot of decoded images\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "# Output the grid of faces\n",
    "for i in range(grid_width * grid_height):\n",
    "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(generated_imgs[i][0].detach().cpu(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452150a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs = None\n",
    "for i, sample_batch in enumerate(lego_dataloader):\n",
    "    if i == 0:\n",
    "        total_imgs = sample_batch\n",
    "    else:\n",
    "        total_imgs = torch.cat([total_imgs, sample_batch], dim=0)\n",
    "print(total_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8db29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_width, grid_height = (5, 3)\n",
    "z_samples = torch.randn(size=(grid_width * grid_height, Z_DIM), device=DEVICE)\n",
    "dcgan.eval()\n",
    "with torch.no_grad():\n",
    "    generated_imgs = dcgan.generate(z_samples).detach().cpu()\n",
    "\n",
    "# Draw a plot of decoded images\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "fig.suptitle('Generated images')\n",
    "\n",
    "# Output the grid of faces\n",
    "for i in range(grid_width * grid_height):\n",
    "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(generated_imgs[i][0], cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
